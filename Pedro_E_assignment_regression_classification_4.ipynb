{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pedro E assignment_regression_classification_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedroescobedob/DS-Unit-2-Linear-Models/blob/master/Pedro_E_assignment_regression_classification_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maDJWkyQ-v8J",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 1, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7IXUfiQ2UKj6"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "\n",
        "## Assignment ðŸŒ¯\n",
        "\n",
        "You'll use a [**dataset of 400+ burrito reviews**](https://srcole.github.io/100burritos/). How accurately can you predict whether a burrito is rated 'Great'?\n",
        "\n",
        "> We have developed a 10-dimensional system for rating the burritos in San Diego. ... Generate models for what makes a burrito great and investigate correlations in its dimensions.\n",
        "\n",
        "- [ ] Do train/validate/test split. Train on reviews from 2016 & earlier. Validate on 2017. Test on 2018 & later.\n",
        "- [ ] Begin with baselines for classification.\n",
        "- [ ] Use scikit-learn for logistic regression.\n",
        "- [ ] Get your model's validation accuracy. (Multiple times if you try multiple iterations.)\n",
        "- [ ] Get your model's test accuracy. (One time, at the end.)\n",
        "- [ ] Commit your notebook to your fork of the GitHub repo.\n",
        "- [ ] Watch Aaron's [video #1](https://www.youtube.com/watch?v=pREaWFli-5I) (12 minutes) & [video #2](https://www.youtube.com/watch?v=bDQgVt4hFgY) (9 minutes) to learn about the mathematics of Logistic Regression.\n",
        "\n",
        "\n",
        "## Stretch Goals\n",
        "\n",
        "- [ ] Add your own stretch goal(s) !\n",
        "- [ ] Make exploratory visualizations.\n",
        "- [ ] Do one-hot encoding.\n",
        "- [ ] Do [feature scaling](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
        "- [ ] Get and plot your coefficients.\n",
        "- [ ] Try [scikit-learn pipelines](https://scikit-learn.org/stable/modules/compose.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o9eSnDYhUGD7",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bVXIApW-v8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data downloaded from https://srcole.github.io/100burritos/\n",
        "import pandas as pd\n",
        "df = pd.read_csv(DATA_PATH+'burritos/burritos.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puTeJZ6z-v8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Derive binary classification target:\n",
        "# We define a 'Great' burrito as having an\n",
        "# overall rating of 4 or higher, on a 5 point scale.\n",
        "# Drop unrated burritos.\n",
        "df = df.dropna(subset=['overall'])\n",
        "df['Great'] = df['overall'] >= 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETUUPU8V-v8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean/combine the Burrito categories\n",
        "df['Burrito'] = df['Burrito'].str.lower()\n",
        "\n",
        "california = df['Burrito'].str.contains('california')\n",
        "asada = df['Burrito'].str.contains('asada')\n",
        "surf = df['Burrito'].str.contains('surf')\n",
        "carnitas = df['Burrito'].str.contains('carnitas')\n",
        "\n",
        "df.loc[california, 'Burrito'] = 'California'\n",
        "df.loc[asada, 'Burrito'] = 'Asada'\n",
        "df.loc[surf, 'Burrito'] = 'Surf & Turf'\n",
        "df.loc[carnitas, 'Burrito'] = 'Carnitas'\n",
        "df.loc[~california & ~asada & ~surf & ~carnitas, 'Burrito'] = 'Other'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ejjkvck-v8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop some high cardinality categoricals\n",
        "df = df.drop(columns=['Notes', 'Location', 'Reviewer', 'Address', 'URL', 'Neighborhood'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6tZ8wpQ-v8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop some columns to prevent \"leakage\"\n",
        "df = df.drop(columns=['Rec', 'overall'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW6z0mcF-v8t",
        "colab_type": "code",
        "outputId": "7cda22ba-9695-4ede-e46c-5ce3ef26ef36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Burrito</th>\n",
              "      <th>Date</th>\n",
              "      <th>Yelp</th>\n",
              "      <th>Google</th>\n",
              "      <th>Chips</th>\n",
              "      <th>Cost</th>\n",
              "      <th>Hunger</th>\n",
              "      <th>Mass (g)</th>\n",
              "      <th>Density (g/mL)</th>\n",
              "      <th>Length</th>\n",
              "      <th>Circum</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Tortilla</th>\n",
              "      <th>Temp</th>\n",
              "      <th>Meat</th>\n",
              "      <th>Fillings</th>\n",
              "      <th>Meat:filling</th>\n",
              "      <th>Uniformity</th>\n",
              "      <th>Salsa</th>\n",
              "      <th>Synergy</th>\n",
              "      <th>Wrap</th>\n",
              "      <th>Unreliable</th>\n",
              "      <th>NonSD</th>\n",
              "      <th>Beef</th>\n",
              "      <th>Pico</th>\n",
              "      <th>Guac</th>\n",
              "      <th>Cheese</th>\n",
              "      <th>Fries</th>\n",
              "      <th>Sour cream</th>\n",
              "      <th>Pork</th>\n",
              "      <th>Chicken</th>\n",
              "      <th>Shrimp</th>\n",
              "      <th>Fish</th>\n",
              "      <th>Rice</th>\n",
              "      <th>Beans</th>\n",
              "      <th>Lettuce</th>\n",
              "      <th>Tomato</th>\n",
              "      <th>Bell peper</th>\n",
              "      <th>Carrots</th>\n",
              "      <th>Cabbage</th>\n",
              "      <th>Sauce</th>\n",
              "      <th>Salsa.1</th>\n",
              "      <th>Cilantro</th>\n",
              "      <th>Onion</th>\n",
              "      <th>Taquito</th>\n",
              "      <th>Pineapple</th>\n",
              "      <th>Ham</th>\n",
              "      <th>Chile relleno</th>\n",
              "      <th>Nopales</th>\n",
              "      <th>Lobster</th>\n",
              "      <th>Queso</th>\n",
              "      <th>Egg</th>\n",
              "      <th>Mushroom</th>\n",
              "      <th>Bacon</th>\n",
              "      <th>Sushi</th>\n",
              "      <th>Avocado</th>\n",
              "      <th>Corn</th>\n",
              "      <th>Zucchini</th>\n",
              "      <th>Great</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>California</td>\n",
              "      <td>1/18/2016</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.49</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>California</td>\n",
              "      <td>1/24/2016</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.45</td>\n",
              "      <td>3.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Carnitas</td>\n",
              "      <td>1/24/2016</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.85</td>\n",
              "      <td>1.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Asada</td>\n",
              "      <td>1/24/2016</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.25</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>California</td>\n",
              "      <td>1/27/2016</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>x</td>\n",
              "      <td>6.59</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Burrito       Date  Yelp  Google Chips  ...  Sushi  Avocado  Corn  Zucchini  Great\n",
              "0  California  1/18/2016   3.5     4.2   NaN  ...    NaN      NaN   NaN       NaN  False\n",
              "1  California  1/24/2016   3.5     3.3   NaN  ...    NaN      NaN   NaN       NaN  False\n",
              "2    Carnitas  1/24/2016   NaN     NaN   NaN  ...    NaN      NaN   NaN       NaN  False\n",
              "3       Asada  1/24/2016   NaN     NaN   NaN  ...    NaN      NaN   NaN       NaN  False\n",
              "4  California  1/27/2016   4.0     3.8     x  ...    NaN      NaN   NaN       NaN   True\n",
              "\n",
              "[5 rows x 59 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53Ky1vgM_S-m",
        "colab_type": "code",
        "outputId": "4c2205f5-f7fa-49e8-fb50-24a5c19a3b27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(df.shape)\n",
        "print(df.dtypes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(421, 59)\n",
            "Burrito            object\n",
            "Date               object\n",
            "Yelp              float64\n",
            "Google            float64\n",
            "Chips              object\n",
            "Cost              float64\n",
            "Hunger            float64\n",
            "Mass (g)          float64\n",
            "Density (g/mL)    float64\n",
            "Length            float64\n",
            "Circum            float64\n",
            "Volume            float64\n",
            "Tortilla          float64\n",
            "Temp              float64\n",
            "Meat              float64\n",
            "Fillings          float64\n",
            "Meat:filling      float64\n",
            "Uniformity        float64\n",
            "Salsa             float64\n",
            "Synergy           float64\n",
            "Wrap              float64\n",
            "Unreliable         object\n",
            "NonSD              object\n",
            "Beef               object\n",
            "Pico               object\n",
            "Guac               object\n",
            "Cheese             object\n",
            "Fries              object\n",
            "Sour cream         object\n",
            "Pork               object\n",
            "Chicken            object\n",
            "Shrimp             object\n",
            "Fish               object\n",
            "Rice               object\n",
            "Beans              object\n",
            "Lettuce            object\n",
            "Tomato             object\n",
            "Bell peper         object\n",
            "Carrots            object\n",
            "Cabbage            object\n",
            "Sauce              object\n",
            "Salsa.1            object\n",
            "Cilantro           object\n",
            "Onion              object\n",
            "Taquito            object\n",
            "Pineapple          object\n",
            "Ham                object\n",
            "Chile relleno      object\n",
            "Nopales            object\n",
            "Lobster            object\n",
            "Queso             float64\n",
            "Egg                object\n",
            "Mushroom           object\n",
            "Bacon              object\n",
            "Sushi              object\n",
            "Avocado            object\n",
            "Corn               object\n",
            "Zucchini           object\n",
            "Great                bool\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1QA-WB3_qNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract year from date column, create column\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format = True)\n",
        "df['year'] = df['Date'].dt.year"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loy8BMNNAAkg",
        "colab_type": "code",
        "outputId": "983cf311-7057-4898-e96f-51140a25c7d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "df.year.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2016    296\n",
              "2017     85\n",
              "2018     27\n",
              "2019     10\n",
              "2026      1\n",
              "2015      1\n",
              "2011      1\n",
              "Name: year, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5DggjpCAPnq",
        "colab_type": "code",
        "outputId": "2339f740-2ab4-4eb3-b8f8-295be0e09051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Train, test, validate\n",
        "\n",
        "train = df[df['year'] < 2017]\n",
        "validate = df[df['year'] == 2017]\n",
        "test = df[df['year'] > 2017]\n",
        "\n",
        "train.shape, test.shape, validate.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((298, 60), (38, 60), (85, 60))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFYvv2pNA9h3",
        "colab_type": "text"
      },
      "source": [
        "# **Baselines for Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DR3oedwBNIO",
        "colab_type": "code",
        "outputId": "dc27dbd7-0646-4499-d10c-15bb08d34194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "target = 'Great'\n",
        "y_train = train[target]\n",
        "y_train.value_counts(normalize = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False    0.590604\n",
              "True     0.409396\n",
              "Name: Great, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh2w5nQ3CBLL",
        "colab_type": "code",
        "outputId": "b77563df-2544-42de-b90b-d3c7d203659a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Majority class for every prediction\n",
        "majority_class = y_train.mode()[0]\n",
        "y_pred = [majority_class] * len(y_train)\n",
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False,\n",
              " False]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjydUFpjCOD3",
        "colab_type": "code",
        "outputId": "147478c5-7a0d-47e9-f031-cbdf993401a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Baseline accuracy for guessing majority class for every prediction\n",
        "from sklearn.metrics import accuracy_score as AS \n",
        "AS(y_train, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5906040268456376"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmrEM5uxCSbS",
        "colab_type": "code",
        "outputId": "e7a5e112-5004-4e41-bfab-68bfbb354ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Accuracy score for validated set\n",
        "y_val = validate[target]\n",
        "y_pred = [majority_class] * len(y_val)\n",
        "AS(y_val, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5529411764705883"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8cyPGRdkF0o",
        "colab_type": "code",
        "outputId": "8d139c19-4f65-4538-a187-45dc8b5e4339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Validation Accuracy\n",
        "import category_encoders as ce\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "target = 'Great'\n",
        "features = ['Tortilla','Temp','Meat','Fillings']\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "\n",
        "X_val = validate[features]\n",
        "y_val = validate[target]\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = ce.OneHotEncoder(use_cat_names = True)\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "X_val_encoded = encoder.transform(X_val)\n",
        "\n",
        "# Imputing missing values\n",
        "imputer = SimpleImputer()\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_val_imputed = imputer.transform(X_val)\n",
        "\n",
        "# Scaling features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "X_val_scaled = scaler.transform(X_val_imputed)\n",
        "\n",
        "# Find Validation Accuracy\n",
        "model = LogisticRegression(random_state = 42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print ('Validation Accuracy', model.score(X_val_scaled,y_val))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 0.8705882352941177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpVK81wopM2p",
        "colab_type": "code",
        "outputId": "6e0e18b8-4b4d-41d1-9946-0e41b1bfa77a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Find Test Accuracy\n",
        "\n",
        "X_test = test[features]\n",
        "y_test = test[target]\n",
        "\n",
        "X_test_encoded = encoder.transform(X_test)\n",
        "X_test_imputed = imputer.transform(X_test_encoded)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "print('Test Accuracy', model.score(X_test_scaled, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.7105263157894737\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}